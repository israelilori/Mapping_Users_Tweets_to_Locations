---
title: "Sentiment_Polarity_keyword"
  html_document: default
  pdf_document: default
---

## Cleaning keyword search data for Sentiment Polarity analysis (including retweets)
This section documents the steps taken for cleaning the Tweets resulting from the keyword search to prepare for the Sentiment Polarity analysis, which included retweets. The following cleaning steps were applied
- Filter out tweets not written in English (as we are using an English Lexicon to analyse sentiment)
- Other normal cleaning: lowercase, punctuation, digits, links etc.
- As retweet headers (rt/RT) were not included in the extracted text from the keyword seatch, this cleaning step was not included
- After a manual investigation of a sample of ~15 tweets from the extracted data it was decided to not not filter out tweets which include "Donald Trump Jr", as all of these cases in the sample were also directed towards Donald Trump as well (by using @realDonalTrump or by mentioning him as well as his son). Investigating the other candidates' tweets to see if there seemed to be tweets which were not directed towards them, but other subjects with similar or the same name resulted in the conclusion that this was not an issue as no such cases where found. 

For all the candidates separate datasets the following cleaning steps where performed
```{r setup, include=FALSE}
#loading the libraries
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)
library(dplyr)

#import file to be cleaned and fil 
file<-read.csv("Ocasio_Cortez_keyword.csv")

#extract only columns where the language is english
result <- filter(file, lang == "en")

#choose the text column,  edit text to lowecase and ensure correct coding
tweets.df<-result$text
tweets.df<-tolower(tweets.df)
tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))

tweets.df = gsub("&amp", "", tweets.df) # remove &amp
tweets.df = gsub("@\\w+", "", tweets.df) # remove at people
tweets.df= gsub("[[:punct:]]", "", tweets.df) # remove punctuation
tweets.df = gsub("[[:digit:]]", "", tweets.df) # remove numbers
tweets.df = gsub("http\\w+", "", tweets.df) # remove html links
# remove unnecessary spaces
tweets.df = gsub("[ \t]{2,}", "", tweets.df)
tweets.df= gsub("^\\s+|\\s+$", "", tweets.df)

#save cleaned data
write.csv(tweets.df, file="Ocasio_Cortez_keyword_clean_inclretweets.csv")
```

## Sentiment Polarity Analysis (including retweets)
This section performs a sentiment polarity analysis on the extracted data from the keyword search for each candidate, here including the retweets. 

The process below had to be performed for each candidates dataset.

```{r setup, include=FALSE}
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)

#read clean data with retweets
spa_retweet <-read.csv("Ocasio_Cortez_keyword_clean_inclretweets.csv")

#extract only text columm (x)
tweets.df<-spa_retweet$x

#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")

#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
  # Parameters
  # sentences: vector of text to score
  # pos.words: vector of words of postive sentiment
  # neg.words: vector of words of negative sentiment
  # .progress: passed to laply() to control of progress bar
  
  # create simple array of scores with laply
  scores <- laply(sentences,
                  function(sentence, pos.words, neg.words)
                  {
                    # remove punctuation
                    sentence <- gsub("[[:punct:]]", "", sentence)
                    # remove control characters
                    sentence <- gsub("[[:cntrl:]]", "", sentence)
                    # remove digits
                    sentence <- gsub('\\d+', '', sentence)
                    
                    #convert to lower
                    sentence <- tolower(sentence)
                    
                    
                    # split sentence into words with str_split (stringr package)
                    word.list <- str_split(sentence, "\\s+")
                    words <- unlist(word.list)
                    
                    # compare words to the dictionaries of positive & negative terms
                    pos.matches <- match(words, pos)
                    neg.matches <- match(words, neg)
                    
                    # get the position of the matched term or NA
                    # we just want a TRUE/FALSE
                    pos.matches <- !is.na(pos.matches)
                    neg.matches <- !is.na(neg.matches)
                    
                    # final score
                    score <- sum(pos.matches) - sum(neg.matches)
                    return(score)
                  }, pos.words, neg.words, .progress=.progress )
  # data frame with scores for each sentence
  scores.df <- data.frame(text=sentences, score=scores)
  return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(tweets.df, pos.txt, neg.txt, .progress='text')
View(scores_twitter)

#Summary of the sentiment scores
summary(scores_twitter)

scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negative', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))
View(scores_twitter)


#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")  

#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels = percent)+labs(y="Score")+
  theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1 + ggtitle("Ocasio-Cortez Sentiment Polarity (including retweets) ") + theme(plot.title = element_text(hjust = 0.5))

```

## Cleaning keyword search data for Sentiment Polarity analysis (excluding retweets)
This section documents the steps taken for cleaning the Tweets resulting from the keyword search to prepare for the Sentiment Polarity analysis, NOT including retweets. The following cleaning steps were applied
- Filter out tweets not written in English (as we are using an English Lexicon to analyse sentiment)
- Filter out tweets which where retweets (a column in the dataset)
- Other normal cleaning: lowercase, punctuation, digits, links etc.
- As retweet headers (rt/RT) were not included in the extracted text from the keyword seatch, this cleaning step was not included
- After a manual investigation of a sample of ~15 tweets from the extracted data it was decided to not not filter out tweets which include "Donald Trump Jr", as all of these cases in the sample were also directed towards Donald Trump as well (by using @realDonalTrump or by mentioning him as well as his son). Investigating the other candidates' tweets to see if there seemed to be tweets which were not directed towards them, but other subjects with similar or the same name resulted in the conclusion that this was not an issue as no such cases where found. 

The process below had to be performed for each candidates' dataset.

```{r setup, include=FALSE}
#loading the libraries
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)
library(dplyr)

#import file to be cleaned and file 
file<-read.csv("Ocasio_Cortez_keyword.csv")

#extract only columns where the language is english and which are not retweets
result <- filter(file, lang == "en" & is_retweet == "FALSE")

#choose the text column,  edit text to lowecase and ensure correct coding
tweets.df<-result$text
tweets.df<-tolower(tweets.df)
tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))

tweets.df = gsub("&amp", "", tweets.df) # remove &amp
tweets.df = gsub("@\\w+", "", tweets.df) # remove at people
tweets.df= gsub("[[:punct:]]", "", tweets.df) # remove punctuation
tweets.df = gsub("[[:digit:]]", "", tweets.df) # remove numbers
tweets.df = gsub("http\\w+", "", tweets.df) # remove html links
# remove unnecessary spaces
tweets.df = gsub("[ \t]{2,}", "", tweets.df)
tweets.df= gsub("^\\s+|\\s+$", "", tweets.df)

#save cleaned data as csv
write.csv(tweets.df, file="Ocasio_Cortez_keyword_clean_NOretweets.csv")

```
## Sentiment Polarity Analysis (WITHOUT retweets)
This section performs a sentiment polarity analysis on the extracted data from the keyword search for each candidate, NOT including the retweets. 

The process below had to be performed for each candidates dataset.

```{r setup, include=FALSE}

library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)

#read clean data with retweets
spa_retweet <-read.csv("Ocasio_Cortez_keyword_clean_NOretweets.csv")
tweets.df<-spa_retweet$x


#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")

#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
  # Parameters
  # sentences: vector of text to score
  # pos.words: vector of words of postive sentiment
  # neg.words: vector of words of negative sentiment
  # .progress: passed to laply() to control of progress bar
  
  # create simple array of scores with laply
  scores <- laply(sentences,
                  function(sentence, pos.words, neg.words)
                  {
                    # remove punctuation
                    sentence <- gsub("[[:punct:]]", "", sentence)
                    # remove control characters
                    sentence <- gsub("[[:cntrl:]]", "", sentence)
                    # remove digits
                    sentence <- gsub('\\d+', '', sentence)
                    
                    #convert to lower
                    sentence <- tolower(sentence)
                    
                    
                    # split sentence into words with str_split (stringr package)
                    word.list <- str_split(sentence, "\\s+")
                    words <- unlist(word.list)
                    
                    # compare words to the dictionaries of positive & negative terms
                    pos.matches <- match(words, pos)
                    neg.matches <- match(words, neg)
                    
                    # get the position of the matched term or NA
                    # we just want a TRUE/FALSE
                    pos.matches <- !is.na(pos.matches)
                    neg.matches <- !is.na(neg.matches)
                    
                    # final score
                    score <- sum(pos.matches) - sum(neg.matches)
                    return(score)
                  }, pos.words, neg.words, .progress=.progress )
  # data frame with scores for each sentence
  scores.df <- data.frame(text=sentences, score=scores)
  return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(tweets.df, pos.txt, neg.txt, .progress='text')


View(scores_twitter)

#Summary of the sentiment scores
summary(scores_twitter)

scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negative', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))


View(scores_twitter)


#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")  

#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels = percent)+labs(y="Score")+
  theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1 + ggtitle("Ocasio-Cortez Sentiment Polarity (without retweets) ") + theme(plot.title = element_text(hjust = 0.5))

```

